"""
File name: LLMController.py
Author: Nathan Foucher 
Contact: nathan.foucher@ext.uni.lu
Created: 30/09/2024
Version: 1.0
Description: File where is defined the abstract class to communicate with the LLMs. You need to define your own implementations of the abstracts methods in this file. 
"""

from abc import ABC, abstractmethod
import subprocess
import litellm
import os
import ollama
# from llama_cpp import Llama
import json
from openai import OpenAI
import time

MAX_TOKEN = 200


class LLMController(ABC):
    """
    Abstract class defining the interface to interact with a LLM model.
    """

    @abstractmethod
    def __init__(self, api_key: str = None, extra=None, hostname=None,port=None):
        """
        Initializes the class with the API key if needed.

        Args:
            api_key (str): The API key.
            extra (str): extra input maybe used by the controller.
        """
        pass

    @abstractmethod
    def askPrompt(self, prompt: str) -> str:
        """
        Abstract method that must be implemented to ask a question (prompt) to a language model.

        Args:
            prompt (str): The question to ask the LLM model.

        Returns:
            str: The response generated by the LLM model.
        """
        pass

class vLLM(LLMController):
    def __init__(self, api_key: str = "EMPTY", extra=None, hostname=None,port=None):
        """
        Utilise un modèle vllm
        Args:
            model_name (str): Le nom du modèle avec vLLM
        """
        self.model = str(extra)
        print("http://"+str(hostname)+":"+str(port)+"/v1")
        self.client = OpenAI(
            api_key=api_key,
            base_url="http://"+hostname+":"+port+"/v1"
        )
        

    

    def askPrompt(self, prompt: str) -> str:
        """
        Pose une question au modèle et renvoie la réponse.

        Args:
            prompt (str): La question à poser au modèle LLM.

        Returns:
            str: La réponse générée par le modèle.
        """
        completion = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "user",
                    "content": str(prompt)
                }
            ], max_tokens=MAX_TOKEN
        )
        return str(completion.choices[0].message.content)

class OpenAi(LLMController):
    def __init__(self, api_key=None, extra=None,hostname=None,port=None):
        """
        Initialise le modèle Llama 2 en utilisant le wrapper litellm.

        Args:
            model_name (str): Le model a utiliser
            api_key (str): Le jeton d'API pour OPEN AI
        """
        if api_key :
            os.environ["OPENAI_API_KEY"] = api_key
            self.model = extra
            
        else : 
            raise Exception("OPEN AI API key is missing. Please provide one in the .env file.")

        #If a sepcifc server is specified, we use it (for vLLM use). Otherwise defaut OpenAI access
        if hostname:
            self.client = OpenAI(
            #api_key=api_key,
                base_url="http://"+hostname+":"+port+"/v1"
            )
        else:
            self.client = OpenAI()
      



    def askPrompt(self, prompt: str) -> str:
        """
        Pose une question au modèle et renvoie la réponse.

        Args:
            prompt (str): La question à poser au modèle LLM.

        Returns:
            str: La réponse générée par le modèle.
        """
        completion = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "user",
                    "content": str(prompt)
                }
            ], max_tokens=MAX_TOKEN
        )
        return str(completion.choices[0].message.content)

class GroundTruth(LLMController):
    """
    Concrete class that interacts with a predefined dataset of prompts and responses stored in a JSON file.
    """

    def __init__(self, api_key: str = None, extra: str = None):
        """
        Initialize the GroundTruth with a path to a JSON file containing prompt-response pairs.

        Args:
            api_key: Not used.
            extra: Path to the JSON file.
        """
        super().__init__(api_key, extra)

        # Load the JSON file
        self.data = []
        if extra:
            self.load_json(extra)

    def load_json(self, file_path: str):
        """
        Loads the dataset from the specified JSON file.

        Args:
            file_path (str): Path to the JSON file.
        """

        with open(file_path, 'r') as f:
            self.data = json.load(f)

            

    def askPrompt(self, prompt: str) -> str:
        """
        Searches for a prompt in the dataset and returns the associated response.

        Args:
            prompt (str): The question to ask the LLM model.

        Returns:
            str: The response associated with the prompt.
        """
        for request in self.data.get("requests", []):
            if request["question"] == prompt:
                return request["response"]

        return "*Prompt not found in dataset.*"




class HF(LLMController):
    def __init__(self, api_key=None, extra=None):
        """
        Initialise le modèle Llama 2 en utilisant le wrapper litellm.

        Args:
            model_name (str): Le nom du modèle à utiliser depuis Hugging Face.
            api_key (str): Le jeton d'API pour Hugging Face.
        """
        if api_key is None:
            raise ValueError("Hugging API key is missing. Please provide one in the .env file.")
        os.environ["HUGGINGFACE_API_KEY"] = api_key
        self.model_name = "huggingface/" + str(extra)
        subprocess.run(["huggingface-cli", "login", "--token", api_key])

        

    def askPrompt(self, prompt: str) -> str:
        """
        Pose une question au modèle et renvoie la réponse.

        Args:
            prompt (str): La question à poser au modèle LLM.

        Returns:
            str: La réponse générée par le modèle.
        """
        messages = [{"content": str(prompt), "role": "user"}]
        response = litellm.completion(model=self.model_name, messages=messages, max_tokens=MAX_TOKEN)

        # Retourner la réponse générée
        return str(response["choices"][0].message.content)


class Ollama(LLMController):
    """
    Classe pour utilisation de Llama 2 en local
    """

    def __init__(self, api_key: str, extra):
        self.model = str(extra)
        pass

    def askPrompt(self, prompt: str) -> str:
        response = ollama.generate(
            model=self.model,
            prompt=prompt,
            options=dict(seed=42),
        )
        return response["response"]


# class quantized_llama2_7b(LLMController):
#     def __init__(self, api_key: str, extra):
#         # Put the location of to the GGUF model that you've download from HuggingFace here
#         self.model_path = extra
#         self.llm = Llama(model_path=self.model_path)

#     def askPrompt(self, prompt: str) -> str:
#         question = f"""<s>[INST]
#         {prompt} [/INST]"""

#         # Run the model
#         output = self.llm(
#             question,  # Prompt
#             max_tokens=32,  # Generate up to 32 tokens
#             stop=[
#                 "Q:",
#                 "\n",
#             ],  # Stop generating just before the model would generate a new question
#             echo=True,  # Echo the prompt back in the output
#         )  # Generate a completion, can also call create_completion

#         return str(output["choices"][0]["text"].split("[/INST]", 1)[1])


# class quantized_mistral_7b(LLMController):
#     def __init__(self, api_key: str, extra):
#         # Put the location of to the GGUF model that you've download from HuggingFace here
#         self.model_path = extra
#         self.llm = Llama(model_path=self.model_path)

#     def askPrompt(self, prompt: str) -> str:
#         question = f"Q: {prompt} A: "

#         output = self.llm(
#             question,
#             max_tokens=32,
#             stop=["Q:", "\n"],
#             echo=True
#             )
#         print(output)


#         return str(output["choices"][0]["text"].split("A:", 1)[1])

# class quantized_llama3_8b(LLMController):
#     def __init__(self, api_key: str, extra):
#         # Put the location of to the GGUF model that you've download from HuggingFace here
#         self.model_path = extra
#         self.llm = Llama(model_path=self.model_path)

#     def askPrompt(self, prompt: str) -> str:
#         question = f"Q: {prompt} A: "

#         question = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"

#         output = self.llm(
#             question,
#             max_tokens=32,
#             stop=["<|end_header_id|>"],
#             echo=True
#             )


#         return str(output["choices"][0]["text"].split("<|end_header_id|>", 1)[1])
    
